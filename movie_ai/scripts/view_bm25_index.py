"""
æŸ¥çœ‹ BM25 ç´¢å¼•å†…å®¹
"""
import os
import sys
import pickle

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.config import Config


def view_bm25_index(cache_path: str = None):
    """æŸ¥çœ‹ BM25 ç´¢å¼•å†…å®¹

    Args:
        cache_path: ç´¢å¼•ç¼“å­˜æ–‡ä»¶è·¯å¾„
    """
    if cache_path is None:
        cache_path = Config.BM25_CACHE_FILE

    if not os.path.exists(cache_path):
        print(f"âŒ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {cache_path}")
        return

    print("=" * 60)
    print("BM25 ç´¢å¼•å†…å®¹æŸ¥çœ‹")
    print("=" * 60)

    # åŠ è½½ç´¢å¼•
    print(f"\nğŸ“‚ åŠ è½½ç´¢å¼•: {cache_path}")
    with open(cache_path, 'rb') as f:
        cache_data = pickle.load(f)

    bm25 = cache_data['bm25']
    doc_ids = cache_data['doc_ids']
    doc_texts = cache_data['doc_texts']
    tokenized_corpus = cache_data.get('tokenized_corpus')

    print(f"âœ… åŠ è½½æˆåŠŸï¼")
    print(f"   - æ–‡æ¡£æ•°é‡: {len(doc_ids)}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {len(bm25.idf)}")

    # æ˜¾ç¤ºå‰5ä¸ªæ–‡æ¡£
    print("\n" + "=" * 60)
    print("å‰ 5 ä¸ªæ–‡æ¡£ç¤ºä¾‹:")
    print("=" * 60)
    for i in range(min(5, len(doc_ids))):
        print(f"\nã€æ–‡æ¡£ {i + 1}ã€‘")
        print(f"ID: {doc_ids[i]}")
        print(f"æ–‡æœ¬: {doc_texts[i][:100]}..." if len(doc_texts[i]) > 100 else f"æ–‡æœ¬: {doc_texts[i]}")

    # æ˜¾ç¤ºè¯æ±‡è¡¨ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("è¯æ±‡è¡¨ç»Ÿè®¡:")
    print("=" * 60)
    idf_scores = bm25.idf
    sorted_vocab = sorted(idf_scores.items(), key=lambda x: x[1], reverse=True)

    print(f"\nIDF æœ€é«˜çš„ 10 ä¸ªè¯:")
    for word, idf in sorted_vocab[:10]:
        print(f"   {word}: {idf:.3f}")

    print(f"\nIDF æœ€ä½çš„ 10 ä¸ªè¯:")
    for word, idf in sorted_vocab[-10:]:
        print(f"   {word}: {idf:.3f}")

    # è¯é¢‘ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("è¯é¢‘ç»Ÿè®¡:")
    print("=" * 60)
    if tokenized_corpus:
        from collections import Counter
        word_freq = Counter()
        for tokens in tokenized_corpus:
            word_freq.update(tokens)

        sorted_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)

        print(f"\nè¯é¢‘æœ€é«˜çš„ 20 ä¸ªè¯:")
        total_words = sum(word_freq.values())
        for word, freq in sorted_freq[:20]:
            percentage = (freq / total_words) * 100
            print(f"   {word}: {freq} æ¬¡ ({percentage:.2f}%)")

        print(f"\nè¯æ±‡æ€»æ•°: {total_words}")
        print(f"ä¸åŒè¯æ•°: {len(word_freq)}")
    else:
        print("   (æ—§ç¼“å­˜ï¼Œæ— åˆ†è¯æ•°æ®)")

    # åˆ†è¯åçš„æ–‡æ¡£ç¤ºä¾‹
    print("\n" + "=" * 60)
    print("åˆ†è¯ç¤ºä¾‹ (å‰3ä¸ªæ–‡æ¡£):")
    print("=" * 60)
    if tokenized_corpus:
        for i in range(min(3, len(tokenized_corpus))):
            print(f"\næ–‡æ¡£ {i + 1}:")
            tokens = tokenized_corpus[i]
            print(f"   åˆ†è¯ç»“æœ: {tokens}")
            print(f"   è¯æ•°: {len(tokens)}")
    else:
        print("   (æ—§ç¼“å­˜ï¼Œæ— åˆ†è¯æ•°æ®)")

    print("\n" + "=" * 60)


if __name__ == '__main__':
    view_bm25_index()
